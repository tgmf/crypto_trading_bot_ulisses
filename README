That's an excellent suggestion! Testing model consistency through dataset reversal provides valuable insight into robustness. Let me add this to the README:

# Cryptocurrency Trading Bot

A machine learning powered cryptocurrency trading bot focused on quantum-inspired probabilistic decision making.

## Overview

This project implements a sophisticated cryptocurrency trading system using Bayesian and quantum-inspired models to make probabilistic trading decisions. The system is designed with a focus on:

- Fee-aware model training and evaluation
- Probabilistic three-state decision making (long, short, no trade)
- Comprehensive risk management
- Proper train-test separation for reliable performance evaluation

## Project Structure

- **data/** - Data storage
  - **raw/** - Original data from exchanges
  - **processed/** - Feature-engineered data
  - **test_sets/** - Reserved test data for unbiased evaluation
  - **backtest_results/** - Backtest outputs and metrics
  - **walk_forward_results/** - Walk-forward testing results
- **src/** - Source code
  - **data/** - Data collection and management
  - **features/** - Feature engineering
  - **models/** - Trading models and algorithms
  - **backtesting/** - Backtesting framework
  - **visualization/** - Visualization tools
- **notebooks/** - Jupyter notebooks for exploration and analysis
- **config/** - Configuration files
- **models/** - Saved model artifacts
- **logs/** - Application logs

## Setup and Installation

### Prerequisites

- Python 3.8+
- Docker (optional)
- WSL (if on Windows)

### Local Environment Setup

```bash
# Create conda environment
conda create -n trading_env python=3.10 -y
conda activate trading_env

# Install dependencies
pip install -r requirements.txt

# Create necessary directories
mkdir -p data/test_sets
mkdir -p data/walk_forward_results
```

### Docker Setup

```bash
# Build Docker container
docker-compose build

# Start services defined in docker-compose.yml
docker-compose up
```

## Usage

### Using Shell Script

The project includes a convenience script for common operations:

```bash
# Show available commands
./launch.sh help

# Collect historical data from exchanges
./launch.sh collect

# Train models with proper train-test split
./launch.sh train

# Train models with time-series cross-validation
./launch.sh train --cv

# Train with reversed datasets (test model stability)
./launch.sh train --reverse

# Run standard backtests
./launch.sh backtest

# Run walk-forward tests
./launch.sh backtest --walk-forward

# Launch Jupyter notebook for analysis
./launch.sh notebook
```

### Direct Python Usage

```bash
# Activate environment first
conda activate trading_env

# Data Collection
python src/main.py --mode collect --symbols BTC/USDT ETH/USDT --timeframes 1h 4h

# Model Training (with train-test split)
python src/main.py --mode train --symbols BTC/USDT --timeframes 1h

# Model Training (with cross-validation)
python src/main.py --mode train --symbols BTC/USDT --timeframes 1h --cv

# Reverse Train-Test (for model consistency checking)
python src/main.py --mode train --symbols BTC/USDT --timeframes 1h --reverse

# Standard Backtesting
python src/main.py --mode backtest --symbols BTC/USDT --timeframes 1h

# Walk-Forward Testing
python src/main.py --mode backtest --symbols BTC/USDT --timeframes 1h --walk-forward

# Multi-Symbol Testing
python src/main.py --mode backtest --symbols BTC/USDT ETH/USDT --timeframes 1h 4h
```

## Testing Approaches

The system supports multiple testing approaches, each with different strengths:

### Standard Backtesting
- **Use when**: Quickly evaluating a model on historical data
- **Features**: Automatically uses test data when available
- **Command**: `python src/main.py --mode backtest`

### Walk-Forward Testing
- **Use when**: Evaluating how a model performs with periodic retraining
- **Features**: Simulates real-world model deployment with periodic retraining
- **Command**: `python src/main.py --mode backtest --walk-forward`

### Reversed Dataset Testing
- **Use when**: Validating model consistency and robustness
- **Features**: Trains on original test set and tests on original training set
- **Benefits**: Helps detect model instability, dataset peculiarities, or overfitting
- **Command**: `python src/main.py --mode train --reverse`

All approaches support testing on multiple symbols and timeframes to evaluate model robustness across different assets and time horizons.

## Implementation Approach

This project follows a step-by-step development approach:

1. **Phase 1: Setup and Data Collection**
   - Development environment configuration
   - Data collection infrastructure
   - Feature engineering framework

2. **Phase 2: Model Development and Proper Evaluation**
   - Bayesian model implementation with train-test separation
   - Time-series cross-validation
   - Walk-forward testing
   - Dataset reversal validation

3. **Phase 3: Advanced Model Development**
   - Quantum-inspired model implementation
   - Feature importance analysis
   - Hyperparameter optimization

4. **Phase 4: Advanced Strategy and Risk Management**
   - Dynamic position sizing
   - Enhanced risk management
   - Market regime detection

## Key Features

### Bayesian Modeling

The initial model uses Bayesian ordered logistic regression to generate probabilistic trade signals with explicit uncertainty quantification.

### Quantum-Inspired Three-State Framework

The advanced model treats trading decisions as a quantum-inspired three-state system:
- Short (-1)
- No position (0)
- Long (1)

Instead of forced classification, it outputs probabilities for each state, enabling sophisticated hedging strategies when uncertainty is high.

### Fee-Aware Training

All models are trained with explicit incorporation of trading fees, ensuring that signals only trigger when the expected profit exceeds transaction costs plus a minimum profit threshold.

### Proper Model Evaluation

The system enforces proper train-test separation to ensure reliable performance estimates:
- Chronological data splitting (no random splitting)
- Reserved test sets for final evaluation
- Walk-forward testing for simulating real-world deployment
- Dataset reversal testing for consistency validation

## Configuration

Key configuration options in `config/config.yaml`:

```yaml
# Data Collection Settings
data:
  timeframes: [1m, 5m, 15m, 1h, 4h]
  symbols: [BTC/USD, ETH/USD, BTC/USDT, ETH/USDT]
  exchanges: [binance, kraken, kucoin]

# Backtesting Parameters
backtesting:
  fee_rate: 0.0006  # Per-side fee rate
  min_profit_target: 0.008  # Minimum profit threshold

# Walk-Forward Parameters
walk_forward:
  train_window: 180  # Training window in days
  test_window: 30    # Testing window in days
  step_size: 30      # Step size in days
```

## Model Consistency Evaluation

A truly robust trading model should demonstrate consistent performance regardless of the specific dataset used for training and testing. The dataset reversal testing:

1. Trains a model on what was originally designated as the test set
2. Tests this model on what was originally used for training
3. Compares performance metrics between the original and reversed approaches

This approach helps identify:
- **Overfitting**: Large performance discrepancies between the two setups
- **Data Peculiarities**: Special characteristics in either dataset
- **Model Stability**: Consistency of feature importance rankings
- **Regime Shifts**: Performance differences due to different market conditions

A reliable model will show similar (not identical) performance characteristics in both arrangements, while an overfit or unstable model will show dramatic differences.

## License

MIT License